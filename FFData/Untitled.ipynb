{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 2-layer NN\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289da227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_est(est_type,X_pre,Y_pre,X_opt,X_adj,X_t):\n",
    "    # est_type: \"NN1\": 1-layer NN; \"NN2\": 2-layer NN; \"rf\": random forest; \"gb\": gradient boosting\n",
    "    # (X_pre,Y_pre): training data\n",
    "    # X_opt,X_adj,X_t: data used to predict\n",
    "    # output: mean estimator m and the predictions m(X)\n",
    "    if est_type == \"NN1\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN1(input_size=1, output_size=1).to(device)\n",
    "        criterion=nn.MSELoss()\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X_pre))\n",
    "            labels=Variable(torch.from_numpy(Y_pre))\n",
    "            \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=criterion(outputs.float(), labels.float())\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        M_pre = model(torch.from_numpy(X_pre).float())\n",
    "        M_pre = M_pre.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_opt = model(torch.from_numpy(X_opt).float())\n",
    "        M_opt = M_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_adj = model(torch.from_numpy(X_adj).float())\n",
    "        M_adj = M_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_t = model(torch.from_numpy(X_t).float())\n",
    "        M_t = M_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t\n",
    "    if est_type == \"NN2\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN2(input_size=1, output_size=1).to(device)\n",
    "        criterion=nn.MSELoss()\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X_pre))\n",
    "            labels=Variable(torch.from_numpy(Y_pre))\n",
    "    \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=criterion(outputs.float(), labels.float())\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        M_pre = model(torch.from_numpy(X_pre).float())\n",
    "        M_pre = M_pre.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_opt = model(torch.from_numpy(X_opt).float())\n",
    "        M_opt = M_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_adj = model(torch.from_numpy(X_adj).float())\n",
    "        M_adj = M_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_t = model(torch.from_numpy(X_t).float())\n",
    "        M_t = M_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t\n",
    "    if est_type == \"rf\":\n",
    "        model = RandomForestRegressor(n_estimators = 500,random_state=random_seed,criterion='squared_error')\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        M_pre = model.predict(X_pre).reshape(-1,1)\n",
    "        M_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        M_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        M_t = model.predict(X_t).reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t\n",
    "    if est_type == \"gb\":\n",
    "        model = GradientBoostingRegressor(n_estimators=300,random_state=random_seed,loss = \"squared_error\")\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        M_pre = model.predict(X_pre).reshape(-1,1)\n",
    "        M_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        M_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        M_t = model.predict(X_t).reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t\n",
    "    if est_type == \"lin\":\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_pre,Y_pre)\n",
    "        M_pre = model.predict(X_pre).reshape(-1,1)\n",
    "        M_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        M_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        M_t = model.predict(X_t).reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb631f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930462ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
