{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c736aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "import math\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f97af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible estimators\n",
    "\n",
    "# 1-layer NN\n",
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 2-layer NN\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "# Random Forest\n",
    "# model setup: rf = RandomForestRegressor(n_estimators = 500, random_state = 18)\n",
    "# training RF: rf.fit(X_pre, Y_pre)\n",
    "# predict with RF: prediction = rf.predict(X_t)\n",
    "\n",
    "# XGBoost\n",
    "# model setup: xgb = XGBRegressor(n_estimators=300,random_state=0)\n",
    "# training XGBoost: xgb.fit(X_pre,Y_pre)\n",
    "#prediction with XGBoost: xgb.predict(X_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc747a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mean estimator that estimate E[Y|X]\n",
    "def mean_est(est_type,X_pre,Y_pre,X_opt,X_adj,X_t):\n",
    "    # est_type: \"NN1\": 1-layer NN; \"NN2\": 2-layer NN; \"rf\": random forest; \"gb\": gradient boosting\n",
    "    # (X_pre,Y_pre): training data\n",
    "    # X_opt,X_adj,X_t: data used to predict\n",
    "    # output: mean estimator m and the predictions m(X)\n",
    "    if est_type == \"NN1\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN1(input_size=3, output_size=1).to(device)\n",
    "        criterion=nn.MSELoss()\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X_pre))\n",
    "            labels=Variable(torch.from_numpy(Y_pre))\n",
    "            \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=criterion(outputs.float(), labels.float())\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        M_pre = model(torch.from_numpy(X_pre).float())\n",
    "        M_pre = M_pre.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_opt = model(torch.from_numpy(X_opt).float())\n",
    "        M_opt = M_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_adj = model(torch.from_numpy(X_adj).float())\n",
    "        M_adj = M_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_t = model(torch.from_numpy(X_t).float())\n",
    "        M_t = M_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t\n",
    "    if est_type == \"NN2\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN2(input_size=3, output_size=1).to(device)\n",
    "        criterion=nn.MSELoss()\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X_pre))\n",
    "            labels=Variable(torch.from_numpy(Y_pre))\n",
    "    \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=criterion(outputs.float(), labels.float())\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        M_pre = model(torch.from_numpy(X_pre).float())\n",
    "        M_pre = M_pre.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_opt = model(torch.from_numpy(X_opt).float())\n",
    "        M_opt = M_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_adj = model(torch.from_numpy(X_adj).float())\n",
    "        M_adj = M_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        M_t = model(torch.from_numpy(X_t).float())\n",
    "        M_t = M_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return M_pre, M_opt, M_adj, M_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50efba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train estimator that estimate E[(Y-m_0(X))^2|X=x]\n",
    "def var_est(X_pre,Y_pre,M_pre, X_opt,X_adj,X_t,est_type =\"NN1\"):\n",
    "    X = X_pre\n",
    "    Y = (Y_pre-M_pre)**2\n",
    "    if est_type == \"NN1\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN1(input_size=3, output_size=1).to(device)\n",
    "        criterion=nn.MSELoss()\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X))\n",
    "            labels=Variable(torch.from_numpy(Y))\n",
    "            \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=criterion(outputs.float(), labels.float())\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        var_opt = model(torch.from_numpy(X_opt).float())\n",
    "        var_opt = var_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        var_opt = np.maximum(var_opt,0)\n",
    "        var_adj = model(torch.from_numpy(X_adj).float())\n",
    "        var_adj = var_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        var_adj = np.maximum(var_adj,0)\n",
    "        var_t = model(torch.from_numpy(X_t).float())\n",
    "        var_t = var_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        var_t = np.maximum(var_t,0)\n",
    "        return var_opt, var_adj, var_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c524a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train quantile estimator that estimate quantile for Y|X\n",
    "\n",
    "# quantile loss for NN\n",
    "def quantile_loss(preds, target, quantile):\n",
    "    assert not target.requires_grad\n",
    "    assert preds.size(0) == target.size(0)\n",
    "    errors = target - preds\n",
    "    q = quantile\n",
    "    losses = torch.max((q - 1) * errors, q * errors)\n",
    "    loss = torch.sum(losses)\n",
    "    return loss\n",
    "\n",
    "def est_quantile(est_type,quantile,X_pre,Y_pre,X_opt,X_adj,X_t):\n",
    "    # est_type: \"NN1\": 1-layer NN; \"NN2\": 2-layer NN; \"qrf\": quantile regression forest; \"gb\": gradient boostin\n",
    "    # quantile: the quantile we are estimating\n",
    "    # (X_pre,Y_pre): training data\n",
    "    # X_opt,X_adj,X_t: data used to predict\n",
    "    # output: quantile estimator Q and the prediction Q(X)\n",
    "    if est_type == \"NN1\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN1(input_size=3, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X_pre))\n",
    "            labels=Variable(torch.from_numpy(Y_pre))\n",
    "        \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=quantile_loss(outputs, labels, quantile)\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        Q_opt = model(torch.from_numpy(X_opt).float())\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(torch.from_numpy(X_adj).float())\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(torch.from_numpy(X_t).float())\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"NN2\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42) \n",
    "        model = NN2(input_size=3, output_size=1).to(device)\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(1000):\n",
    "            #convert numpy array to torch Variable\n",
    "            inputs=Variable(torch.from_numpy(X_pre))\n",
    "            labels=Variable(torch.from_numpy(Y_pre))\n",
    "        \n",
    "            #clear gradients wrt parameters\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            #Forward to get outputs\n",
    "            outputs=model(inputs.float())\n",
    "    \n",
    "            #calculate loss\n",
    "            loss=quantile_loss(outputs, labels, quantile)\n",
    "    \n",
    "            #getting gradients wrt parameters\n",
    "            loss.backward()\n",
    "    \n",
    "            #updating parameters\n",
    "            optimizer.step()\n",
    "        Q_opt = model(torch.from_numpy(X_opt).float())\n",
    "        Q_opt = Q_opt.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_adj = model(torch.from_numpy(X_adj).float())\n",
    "        Q_adj = Q_adj.detach().cpu().numpy().reshape(-1,1)\n",
    "        Q_t = model(torch.from_numpy(X_t).float())\n",
    "        Q_t = Q_t.detach().cpu().numpy().reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    if est_type == \"qrf\":\n",
    "        model = RandomForestQuantileRegressor(n_estimators = 500, random_state=random_seed)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj,quantiles = [quantile]).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t,quantiles = [quantile]).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t\n",
    "    \n",
    "    \n",
    "    if est_type == \"gb\":\n",
    "        model = GradientBoostingRegressor(n_estimators=300,random_state=random_seed,loss = \"quantile\", alpha = quantile)\n",
    "        model.fit(X_pre, Y_pre)\n",
    "        Q_opt = model.predict(X_opt).reshape(-1,1)\n",
    "        Q_adj = model.predict(X_adj).reshape(-1,1)\n",
    "        Q_t = model.predict(X_t).reshape(-1,1)\n",
    "        return model, Q_opt, Q_adj, Q_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19da89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve RKHS optimization problem\n",
    "def RKHS_opt(K, Y):\n",
    "    n = K.shape[0]\n",
    "    hB = cp.Variable((n, n), symmetric=True)\n",
    "    constraints = [hB >> 0]\n",
    "    constraints += [K[i, :] @ hB @ K[i, :] >= cp.square(Y[i]) for i in range(n)]\n",
    "    prob = cp.Problem(cp.Minimize(cp.trace(K @ hB @ K.T)), constraints)\n",
    "    prob.solve()\n",
    "    return hB.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f3c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving the optimization problem\n",
    "def solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t,\n",
    "              function_class, E_opt=None, E_adj=None, E_t=None, degree = None,sigma = None):\n",
    "    # M_opt: mean estimator m(X_opt)\n",
    "    # function_class: \"aug\": Augmentation, \"rkhs_poly\": RKHS with polynomial kernel, \"rkhs_rbf\": RKHS with RBF kernel\n",
    "    # E_opt: if function_class = \"aug\", then E is the estimator matrix, i.e. (E_opt)_{ij} = f_i((X_opt)_j)\n",
    "    # degree: if function_class = \"rkhs_poly\", then set the degree of the polynomial\n",
    "    # sigma: if function_class = \"rkhs_rbf\", then set parameter sigma\n",
    "    # (X_opt,Y_opt): data used to solve the optimization problem\n",
    "    # output: estimator for 100% coverage V_adj and V_t\n",
    "    n_opt = X_opt.shape[0]\n",
    "    n_adj = X_adj.shape[0]\n",
    "    n_t = X_t.shape[0]\n",
    "    Y = (Y_opt-M_opt)[:,0]\n",
    "        \n",
    "    if function_class == \"aug\":\n",
    "        cons_opt = np.ones(n_opt).reshape(1,-1)\n",
    "        A_opt = np.vstack((E_opt,cons_opt))\n",
    "        weight = cp.Variable(A_opt.shape[0])\n",
    "        constraints = [weight>=0]+[weight @ A_opt >= cp.square(Y)]\n",
    "        prob = cp.Problem(cp.Minimize(cp.sum(weight @ A_opt)), constraints)\n",
    "        prob.solve()\n",
    "        optimal_weight = weight.value\n",
    "        \n",
    "        cons_adj = np.ones(n_adj).reshape(1,-1)\n",
    "        A_adj = np.vstack((E_adj,cons_adj))\n",
    "        V_adj = optimal_weight @ A_adj\n",
    "        V_adj = V_adj.reshape(-1,1)\n",
    "        \n",
    "        cons_t = np.ones(n_t).reshape(1,-1)\n",
    "        A_t = np.vstack((E_t,cons_t))\n",
    "        V_t = optimal_weight @ A_t\n",
    "        V_t = V_t.reshape(-1,1)\n",
    "        return optimal_weight, V_adj, V_t\n",
    "    \n",
    "    if function_class == \"rkhs_poly\":\n",
    "        X_inner_prod = X_opt @ X_opt.T\n",
    "        K = np.power(1 + X_inner_prod, degree)\n",
    "        hB = RKHS_opt(K,Y)\n",
    "        \n",
    "        X_inner_prod_adj = X_adj @ X_opt.T\n",
    "        K_adj = np.power(1 + X_inner_prod_adj, degree)\n",
    "        V_adj = np.diag(K_adj @ hB @ K_adj.T)\n",
    "        V_adj = V_adj.reshape(-1,1)\n",
    "        \n",
    "        X_inner_prod_t = X_t @ X_opt.T\n",
    "        K_t = np.power(1 + X_inner_prod_t, degree)\n",
    "        V_t = np.diag(K_t @ hB @ K_t.T)\n",
    "        V_t = V_t.reshape(-1,1)\n",
    "        return V_adj, V_t\n",
    "    \n",
    "    if function_class == \"rkhs_rbf\":\n",
    "        K_opt = rbf_kernel(X_opt, gamma = 1/(2*sigma**2))\n",
    "        hB = RKHS_opt(K_opt,Y)\n",
    "        \n",
    "        K_adj = rbf_kernel(X_adj, X_opt, gamma = 1/(2*sigma**2))\n",
    "        V_adj = np.diag(K_adj @ hB @ K_adj.T)\n",
    "        V_adj = V_adj.reshape(-1,1)\n",
    "        \n",
    "        K_t = rbf_kernel(X_t, X_opt, gamma = 1/(2*sigma**2))\n",
    "        V_t = np.diag(K_t @ hB @ K_t.T)\n",
    "        V_t = V_t.reshape(-1,1)\n",
    "        return V_adj, V_t\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42dffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval adjustment\n",
    "def interval_adj(X_adj,Y_adj,M_adj,V_adj,alpha, stepsize=0.001,eps=0):\n",
    "    # (X_adj,Y_adj): data used to adjust the interval\n",
    "    # M_adj: mean estimator m(X_adj)\n",
    "    # V_adj: variance estimator for 100% coverage f(X_adj)\n",
    "    # alpha: get 1-alpha coverage\n",
    "    # eps: extend the interval a bit 1/sqrt{log(n_opt)}\n",
    "    # output: adjustment level delta\n",
    "    # the prediction level is: [M-sqrt{delta V}, M+sqrt{delta V}]\n",
    "    I = np.where(V_adj+eps-np.square(Y_adj-M_adj)>=0)[0]\n",
    "    I.tolist()\n",
    "    Y_adj = Y_adj[I].reshape(-1,1)\n",
    "    M_adj = M_adj[I].reshape(-1,1)\n",
    "    V_adj = (V_adj[I]+eps).reshape(-1,1)\n",
    "    delta = 1\n",
    "    prop_outside = (np.power(Y_adj[:,0]-M_adj[:,0], 2) > delta * V_adj[:,0]).mean()\n",
    "    while prop_outside <= alpha:\n",
    "        delta = delta-stepsize\n",
    "        prop_outside = (np.power(Y_adj[:,0]-M_adj[:,0], 2) > delta * V_adj[:,0]).mean()\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03dcaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points uniformly distributed on the surface of a sphere\n",
    "\n",
    "def generate_points_on_sphere(n):\n",
    "    phi = np.random.uniform(0, 2*np.pi, size=n)\n",
    "    cos_theta = np.random.uniform(-1, 1, size=n)\n",
    "    theta = np.arccos(cos_theta)\n",
    "\n",
    "    x = np.sin(theta) * np.cos(phi)\n",
    "    y = np.sin(theta) * np.sin(phi)\n",
    "    z = np.cos(theta)\n",
    "\n",
    "    return np.stack((x, y, z), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4613179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/quantile_forest/_quantile_forest.py:104: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  super(BaseForestQuantileRegressor, self).fit(X, y, sample_weight=sample_weight)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall coverage is 0.95\n",
      "The mean bandwidth for testing data is 4.629806199106926\n"
     ]
    }
   ],
   "source": [
    "# Test 1 (known mean case: m_0=0)\n",
    "\n",
    "# Generate i.i.d data\n",
    "np.random.seed(5)\n",
    "n_pre = 1000\n",
    "n_opt = 500\n",
    "n_adj = 100\n",
    "n_t = 1000\n",
    "n = n_pre+n_opt+n_adj+n_t\n",
    "beta = np.array([1/math.sqrt(3),1/math.sqrt(3),-1/math.sqrt(3)])\n",
    "\n",
    "X = generate_points_on_sphere(n)\n",
    "Y = np.sqrt(1+25*np.power(X @ beta, 4))  * np.random.uniform(-1, 1, n)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_pre = X[0:n_pre,:]\n",
    "Y_pre = Y[0:n_pre,:].reshape(-1, 1)\n",
    "\n",
    "X_opt = X[n_pre:n_pre+n_opt,:]\n",
    "Y_opt = Y[n_pre:n_pre+n_opt,:].reshape(-1, 1)\n",
    "\n",
    "X_adj = X[n_pre+n_opt:n_pre+n_opt+n_adj,:]\n",
    "Y_adj = Y[n_pre+n_opt:n_pre+n_opt+n_adj,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_t = X[n_pre+n_opt+n_adj:,:]\n",
    "Y_t = Y[n_pre+n_opt+n_adj:,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Obtain mean estimator\n",
    "known_mean = \"True\"\n",
    "if known_mean == \"True\":\n",
    "    M_pre = np.zeros(n_pre).reshape(-1,1)\n",
    "    M_opt = np.zeros(n_opt).reshape(-1,1)\n",
    "    M_adj = np.zeros(n_adj).reshape(-1,1)\n",
    "    M_t = np.zeros(n_t).reshape(-1,1)\n",
    "else:\n",
    "    est_type = \"NN2\"\n",
    "    M_pre, M_opt, M_adj, M_t = mean_est(est_type,X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "    \n",
    "# Obtain variance estimator\n",
    "var_opt, var_adj, var_t = var_est(X_pre,Y_pre,M_pre, X_opt,X_adj,X_t,est_type =\"NN1\")\n",
    "    \n",
    "# Obtain quantile estimators\n",
    "quantile = [0.8,0.85,0.9,0.95]\n",
    "# quantile = [0.6,0.7,0.8,0.9]\n",
    "m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "\n",
    "# construct estimator matrix\n",
    "E_opt = np.hstack(((Q1_opt-M_opt)**2, (Q2_opt-M_opt)**2, (Q3_opt-M_opt)**2, (Q4_opt-M_opt)**2, var_opt))\n",
    "E_opt = E_opt.T\n",
    "E_adj = np.hstack(((Q1_adj-M_adj)**2, (Q2_adj-M_adj)**2, (Q3_adj-M_adj)**2, (Q4_adj-M_adj)**2, var_adj))\n",
    "E_adj = E_adj.T\n",
    "E_t = np.hstack(((Q1_t-M_t)**2, (Q2_t-M_t)**2, (Q3_t-M_t)**2, (Q4_t-M_t)**2, var_t))\n",
    "E_t = E_t.T\n",
    "\n",
    "# solve optimization problem\n",
    "\n",
    "optimal_weight, V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"aug\", E_opt, E_adj, E_t)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_poly\", degree = 3)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_rbf\", sigma = 1)\n",
    "\n",
    "# adjust interval\n",
    "alpha = 0.05\n",
    "delta = interval_adj(X_adj,Y_adj,M_adj,V100_adj,alpha)\n",
    "\n",
    "# plot\n",
    "# delta = 1\n",
    "V_alpha_t = delta*V100_t\n",
    "coverage = (np.power(Y_t[:,0]-M_t[:,0], 2) <= V_alpha_t[:,0]).mean()\n",
    "bandwidth = np.mean(V_alpha_t[:,0])\n",
    "print(\"The overall coverage is\", coverage)\n",
    "print(\"The mean bandwidth for testing data is\", bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76ce4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/quantile_forest/_quantile_forest.py:104: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  super(BaseForestQuantileRegressor, self).fit(X, y, sample_weight=sample_weight)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall coverage is 0.964\n",
      "The mean bandwidth for testing data is 5.6548527252807945\n"
     ]
    }
   ],
   "source": [
    "# Test 2 (unknown mean case)\n",
    "\n",
    "# Generate i.i.d data\n",
    "np.random.seed(0)\n",
    "n_pre = 1000\n",
    "n_opt = 1000\n",
    "n_adj = 100\n",
    "n_t = 1000\n",
    "n = n_pre+n_opt+n_adj+n_t\n",
    "beta = np.array([1/math.sqrt(3),1/math.sqrt(3),-1/math.sqrt(3)])\n",
    "\n",
    "X = generate_points_on_sphere(n)\n",
    "Y = 1+5*np.power(X @ beta, 3)+np.sqrt(1+25*np.power(X @ beta, 4))  * np.random.uniform(-1, 1, n)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_pre = X[0:n_pre,:]\n",
    "Y_pre = Y[0:n_pre,:].reshape(-1, 1)\n",
    "\n",
    "X_opt = X[n_pre:n_pre+n_opt,:]\n",
    "Y_opt = Y[n_pre:n_pre+n_opt,:].reshape(-1, 1)\n",
    "\n",
    "X_adj = X[n_pre+n_opt:n_pre+n_opt+n_adj,:]\n",
    "Y_adj = Y[n_pre+n_opt:n_pre+n_opt+n_adj,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_t = X[n_pre+n_opt+n_adj:,:]\n",
    "Y_t = Y[n_pre+n_opt+n_adj:,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Obtain mean estimator\n",
    "known_mean = \"False\"\n",
    "if known_mean == \"True\":\n",
    "    M_pre = np.zeros(n_pre).reshape(-1,1)\n",
    "    M_opt = np.zeros(n_opt).reshape(-1,1)\n",
    "    M_adj = np.zeros(n_adj).reshape(-1,1)\n",
    "    M_t = np.zeros(n_t).reshape(-1,1)\n",
    "else:\n",
    "    est_type = \"NN2\"\n",
    "    M_pre, M_opt, M_adj, M_t = mean_est(est_type,X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "    \n",
    "# Obtain variance estimator\n",
    "var_opt, var_adj, var_t = var_est(X_pre,Y_pre,M_pre, X_opt,X_adj,X_t,est_type =\"NN1\")\n",
    "    \n",
    "# Obtain quantile estimators\n",
    "quantile = [0.8,0.85,0.9,0.95]\n",
    "# quantile = [0.6,0.7,0.8,0.9]\n",
    "m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "\n",
    "# construct estimator matrix\n",
    "E_opt = np.hstack(((Q1_opt-M_opt)**2, (Q2_opt-M_opt)**2, (Q3_opt-M_opt)**2, (Q4_opt-M_opt)**2, var_opt))\n",
    "E_opt = E_opt.T\n",
    "E_adj = np.hstack(((Q1_adj-M_adj)**2, (Q2_adj-M_adj)**2, (Q3_adj-M_adj)**2, (Q4_adj-M_adj)**2, var_adj))\n",
    "E_adj = E_adj.T\n",
    "E_t = np.hstack(((Q1_t-M_t)**2, (Q2_t-M_t)**2, (Q3_t-M_t)**2, (Q4_t-M_t)**2, var_t))\n",
    "E_t = E_t.T\n",
    "\n",
    "# solve optimization problem\n",
    "\n",
    "optimal_weight, V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"aug\", E_opt, E_adj, E_t)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_poly\", degree = 3)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_rbf\", sigma = 1)\n",
    "\n",
    "# adjust interval\n",
    "alpha = 0.05\n",
    "delta = interval_adj(X_adj,Y_adj,M_adj,V100_adj,alpha)\n",
    "\n",
    "# plot\n",
    "V_alpha_t = delta*V100_t\n",
    "coverage = (np.power(Y_t[:,0]-M_t[:,0], 2) <= V_alpha_t[:,0]).mean()\n",
    "bandwidth = np.mean(V_alpha_t[:,0])\n",
    "print(\"The overall coverage is\", coverage)\n",
    "print(\"The mean bandwidth for testing data is\", bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b38b4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/quantile_forest/_quantile_forest.py:104: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  super(BaseForestQuantileRegressor, self).fit(X, y, sample_weight=sample_weight)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall coverage is 0.966\n",
      "The mean bandwidth for testing data is 18.995891507709555\n"
     ]
    }
   ],
   "source": [
    "# Test 3 (\\xi depends on X)\n",
    "\n",
    "# Generate i.i.d data (Y follows a constrained Laplace)\n",
    "np.random.seed(1)\n",
    "n_pre = 1000\n",
    "n_opt = 500\n",
    "n_adj = 100\n",
    "n_t = 1000\n",
    "n = n_pre+n_opt+n_adj+n_t\n",
    "beta = np.array([1/math.sqrt(3),1/math.sqrt(3),-1/math.sqrt(3)])\n",
    "\n",
    "X = generate_points_on_sphere(n)\n",
    "\n",
    "# Specify the mean and standard deviation for Y\n",
    "mean_Y = np.power(X @ beta, 2)+5*np.power(X @ beta, 4)\n",
    "std_dev_Y = np.sqrt(1 + 25 * np.power(X @ beta, 4))\n",
    "mean_Y = mean_Y.reshape(-1,1)\n",
    "std_dev_Y  = std_dev_Y .reshape(-1,1)\n",
    "\n",
    "# Specify the bounds for Y\n",
    "lower_bound = mean_Y - 2 * std_dev_Y\n",
    "upper_bound = mean_Y + 2 * std_dev_Y\n",
    "\n",
    "# Generate all Y values initially\n",
    "Y = np.random.laplace(mean_Y, std_dev_Y)\n",
    "\n",
    "# Correct values that fall out of bounds\n",
    "while True:\n",
    "    out_of_bounds = (Y < lower_bound) | (Y > upper_bound)\n",
    "    if not np.any(out_of_bounds):\n",
    "        break\n",
    "    Y[out_of_bounds] = np.random.laplace(mean_Y[out_of_bounds], std_dev_Y[out_of_bounds])\n",
    "\n",
    "\n",
    "X_pre = X[0:n_pre,:]\n",
    "Y_pre = Y[0:n_pre,:].reshape(-1, 1)\n",
    "\n",
    "X_opt = X[n_pre:n_pre+n_opt,:]\n",
    "Y_opt = Y[n_pre:n_pre+n_opt,:].reshape(-1, 1)\n",
    "\n",
    "X_adj = X[n_pre+n_opt:n_pre+n_opt+n_adj,:]\n",
    "Y_adj = Y[n_pre+n_opt:n_pre+n_opt+n_adj,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_t = X[n_pre+n_opt+n_adj:,:]\n",
    "Y_t = Y[n_pre+n_opt+n_adj:,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Obtain mean estimator\n",
    "known_mean = \"False\"\n",
    "if known_mean == \"True\":\n",
    "    M_pre = np.zeros(n_pre).reshape(-1,1)\n",
    "    M_opt = np.zeros(n_opt).reshape(-1,1)\n",
    "    M_adj = np.zeros(n_adj).reshape(-1,1)\n",
    "    M_t = np.zeros(n_t).reshape(-1,1)\n",
    "else:\n",
    "    est_type = \"NN2\"\n",
    "    M_pre, M_opt, M_adj, M_t = mean_est(est_type,X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "    \n",
    "# Obtain variance estimator\n",
    "var_opt, var_adj, var_t = var_est(X_pre,Y_pre,M_pre, X_opt,X_adj,X_t,est_type =\"NN1\")\n",
    "    \n",
    "# Obtain quantile estimators\n",
    "quantile = [0.8,0.85,0.9,0.95]\n",
    "# quantile = [0.6,0.7,0.8,0.9]\n",
    "m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "\n",
    "# construct estimator matrix\n",
    "E_opt = np.hstack(((Q1_opt-M_opt)**2, (Q2_opt-M_opt)**2, (Q3_opt-M_opt)**2, (Q4_opt-M_opt)**2, var_opt))\n",
    "E_opt = E_opt.T\n",
    "E_adj = np.hstack(((Q1_adj-M_adj)**2, (Q2_adj-M_adj)**2, (Q3_adj-M_adj)**2, (Q4_adj-M_adj)**2, var_adj))\n",
    "E_adj = E_adj.T\n",
    "E_t = np.hstack(((Q1_t-M_t)**2, (Q2_t-M_t)**2, (Q3_t-M_t)**2, (Q4_t-M_t)**2, var_t))\n",
    "E_t = E_t.T\n",
    "\n",
    "# solve optimization problem\n",
    "\n",
    "optimal_weight, V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"aug\", E_opt, E_adj, E_t)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_poly\", degree = 3)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_rbf\", sigma = 1)\n",
    "\n",
    "# adjust interval\n",
    "alpha = 0.05\n",
    "delta = interval_adj(X_adj,Y_adj,M_adj,V100_adj,alpha)\n",
    "\n",
    "# plot\n",
    "V_alpha_t = delta*V100_t\n",
    "coverage = (np.power(Y_t[:,0]-M_t[:,0], 2) <= V_alpha_t[:,0]).mean()\n",
    "bandwidth = np.mean(V_alpha_t[:,0])\n",
    "print(\"The overall coverage is\", coverage)\n",
    "print(\"The mean bandwidth for testing data is\", bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27fdc67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/quantile_forest/_quantile_forest.py:104: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  super(BaseForestQuantileRegressor, self).fit(X, y, sample_weight=sample_weight)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jiawei/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall coverage is 0.967\n",
      "The mean bandwidth for testing data is 79.25937931771708\n"
     ]
    }
   ],
   "source": [
    "# Test 4 (Test 3 without constraint)\n",
    "\n",
    "# Generate i.i.d data (Y follows a constrained Laplace)\n",
    "np.random.seed(1)\n",
    "n_pre = 1000\n",
    "n_opt = 500\n",
    "n_adj = 100\n",
    "n_t = 1000\n",
    "n = n_pre+n_opt+n_adj+n_t\n",
    "beta = np.array([1/math.sqrt(3),1/math.sqrt(3),-1/math.sqrt(3)])\n",
    "\n",
    "X = generate_points_on_sphere(n)\n",
    "\n",
    "# Specify the mean and standard deviation for Y\n",
    "mean_Y = np.power(X @ beta, 2)+5*np.power(X @ beta, 4)\n",
    "std_dev_Y = np.sqrt(1 + 25 * np.power(X @ beta, 4))\n",
    "mean_Y = mean_Y.reshape(-1,1)\n",
    "std_dev_Y  = std_dev_Y .reshape(-1,1)\n",
    "\n",
    "\n",
    "# Generate all Y values initially\n",
    "Y = np.random.laplace(mean_Y, std_dev_Y)\n",
    "\n",
    "\n",
    "X_pre = X[0:n_pre,:]\n",
    "Y_pre = Y[0:n_pre,:].reshape(-1, 1)\n",
    "\n",
    "X_opt = X[n_pre:n_pre+n_opt,:]\n",
    "Y_opt = Y[n_pre:n_pre+n_opt,:].reshape(-1, 1)\n",
    "\n",
    "X_adj = X[n_pre+n_opt:n_pre+n_opt+n_adj,:]\n",
    "Y_adj = Y[n_pre+n_opt:n_pre+n_opt+n_adj,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_t = X[n_pre+n_opt+n_adj:,:]\n",
    "Y_t = Y[n_pre+n_opt+n_adj:,:].reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Obtain mean estimator\n",
    "known_mean = \"False\"\n",
    "if known_mean == \"True\":\n",
    "    M_pre = np.zeros(n_pre).reshape(-1,1)\n",
    "    M_opt = np.zeros(n_opt).reshape(-1,1)\n",
    "    M_adj = np.zeros(n_adj).reshape(-1,1)\n",
    "    M_t = np.zeros(n_t).reshape(-1,1)\n",
    "else:\n",
    "    est_type = \"NN2\"\n",
    "    M_pre, M_opt, M_adj, M_t = mean_est(est_type,X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "    \n",
    "# Obtain variance estimator\n",
    "var_opt, var_adj, var_t = var_est(X_pre,Y_pre,M_pre, X_opt,X_adj,X_t,est_type =\"NN1\")\n",
    "    \n",
    "# Obtain quantile estimators\n",
    "quantile = [0.8,0.85,0.9,0.95]\n",
    "# quantile = [0.6,0.7,0.8,0.9]\n",
    "m1,Q1_opt,Q1_adj,Q1_t = est_quantile(\"NN1\",quantile[0],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m2,Q2_opt,Q2_adj,Q2_t = est_quantile(\"NN2\",quantile[1],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m3,Q3_opt,Q3_adj,Q3_t = est_quantile(\"qrf\",quantile[2],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "m4,Q4_opt,Q4_adj,Q4_t = est_quantile(\"gb\",quantile[3],X_pre,Y_pre,X_opt,X_adj,X_t)\n",
    "\n",
    "# construct estimator matrix\n",
    "E_opt = np.hstack(((Q1_opt-M_opt)**2, (Q2_opt-M_opt)**2, (Q3_opt-M_opt)**2, (Q4_opt-M_opt)**2, var_opt))\n",
    "E_opt = E_opt.T\n",
    "E_adj = np.hstack(((Q1_adj-M_adj)**2, (Q2_adj-M_adj)**2, (Q3_adj-M_adj)**2, (Q4_adj-M_adj)**2, var_adj))\n",
    "E_adj = E_adj.T\n",
    "E_t = np.hstack(((Q1_t-M_t)**2, (Q2_t-M_t)**2, (Q3_t-M_t)**2, (Q4_t-M_t)**2, var_t))\n",
    "E_t = E_t.T\n",
    "\n",
    "# solve optimization problem\n",
    "\n",
    "optimal_weight, V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"aug\", E_opt, E_adj, E_t)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_poly\", degree = 3)\n",
    "#V100_adj, V100_t = solve_opt(X_opt,Y_opt, M_opt, M_adj, M_t, X_adj, X_t, \"rkhs_rbf\", sigma = 1)\n",
    "\n",
    "# adjust interval\n",
    "alpha = 0.05\n",
    "delta = interval_adj(X_adj,Y_adj,M_adj,V100_adj,alpha)\n",
    "\n",
    "# plot\n",
    "V_alpha_t = delta*V100_t\n",
    "coverage = (np.power(Y_t[:,0]-M_t[:,0], 2) <= V_alpha_t[:,0]).mean()\n",
    "bandwidth = np.mean(V_alpha_t[:,0])\n",
    "print(\"The overall coverage is\", coverage)\n",
    "print(\"The mean bandwidth for testing data is\", bandwidth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
